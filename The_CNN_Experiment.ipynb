{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTNdIpvOSQRB"
      },
      "source": [
        "# Setup Experiments [ Models, Epochs ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHReJvS4S9yG",
        "outputId": "d6fa8e8a-6bae-43f2-c822-da6ba5292f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version: 2.8.2\n",
            "Hub version: 0.12.0\n",
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import os\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import h5py\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# dictionary of headless model of different architectures to be used for feature extraction\n",
        "\n",
        "# all from tfhub.dev \n",
        "\n",
        "EPOCHS = 3\n",
        "\n",
        "BATCH_SIZE_LIST = [\"32\", \"48\", \"64\"]\n",
        "\n",
        "models = {\n",
        "    \n",
        "    \"efficientnet\": (\"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\", (224,224)),\n",
        "    \"efficientnet_v2\": (\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\", (224,224)),\n",
        "                     \"\"\n",
        "    \"mobilenet_v2\": (\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\", (224,224)),\n",
        "    \"mobilenet_v3\": (\"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\", (224,224)),\n",
        "\n",
        "    \"inception_v1\":(\"https://tfhub.dev/google/imagenet/inception_v1/feature_vector/5\", (224,224)),\n",
        "    \"inception_v2\":(\"https://tfhub.dev/google/imagenet/inception_v2/feature_vector/5\", (224,224)),\n",
        "\n",
        "    \"inception_v3_inaturalist\":(\"https://tfhub.dev/google/inaturalist/inception_v3/feature_vector/5\", (299,299)), # trained on iNaturalist dataset\n",
        "    \"inception_v3_imagenet\": (\"https://tfhub.dev/google/imagenet/inception_v3/feature_vector/5\", (299,299)),\n",
        "\n",
        "    \"inception_resnet_v2\": (\"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature_vector/5\", (299,299)),\n",
        "\n",
        "\n",
        "    \"resnet_v2\": (\"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5\", (224,224))\n",
        "\n",
        "}\n",
        "\n",
        "for model in models:\n",
        "\n",
        "  print(f\"{model} url --> {models.get(model)[0]} || image size --> {models.get(model)[1]}\")"
      ],
      "metadata": {
        "id": "VSgh_8VcZON5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "671c4fe7-64a7-4bec-e2e8-26a2ce2d5d7f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mobilenet_v2 url --> https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4 || image size --> (224, 224)\n",
            "inception_v1 url --> https://tfhub.dev/google/imagenet/inception_v1/feature_vector/5 || image size --> (224, 224)\n",
            "inception_v3_inaturalist url --> https://tfhub.dev/google/inaturalist/inception_v3/feature_vector/5 || image size --> (299, 299)\n",
            "inception_resnet_v2 url --> https://tfhub.dev/google/imagenet/inception_resnet_v2/feature_vector/5 || image size --> (299, 299)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading The Data"
      ],
      "metadata": {
        "id": "tbTbbJ3QmNxb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR7KggyST3tG",
        "outputId": "be3fd483-520f-4889-ad96-e3e88f4b659c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive and get data\n",
        "from google.colab import drive\n",
        "import pathlib\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/\n",
        "\n",
        "# ^--- paint to where images are"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HnU2pARGdSMV"
      },
      "outputs": [],
      "source": [
        "# Declare some values\n",
        "\n",
        "train_path = './image_data/train'\n",
        "valid_path = './image_data/val'\n",
        "test_path = './image_data/test'\n",
        "\n",
        "# ^-- point to proper directory \n",
        "\n",
        "IMAGE_SIZE = () # <-- variable initionlization only"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to make plots and save\n",
        "\n",
        "def plot_acc(model,dir):\n",
        "  \n",
        "  plt.plot(hist['accuracy'])\n",
        "  plt.plot(hist['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.savefig(dir)\n",
        "  plt.clf()\n",
        "  \n",
        "def plot_loss(model,dir):\n",
        "  \n",
        "  plt.plot(hist['loss'])\n",
        "  plt.plot(hist['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.savefig(dir)\n",
        "  plt.clf()\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "da-vTBMduj7Q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "XZFEWyce97ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = datetime.now() # used to print time taken to run all experiments at end.\n",
        "\n",
        "# loop through all the models\n",
        "\n",
        "for model in models:\n",
        "# initonlizing model and batch size\n",
        "  model_name = model\n",
        "  feature_extractor = models.get(model)[0]\n",
        "  IMAGE_SIZE = models.get(model)[1]\n",
        "  \n",
        "\n",
        "  # loop through all the batch_sizes for each of the models\n",
        "  for BATCH_SIZE in BATCH_SIZE_LIST:\n",
        "    \n",
        "    BATCH_SIZE = int(BATCH_SIZE)\n",
        "\n",
        "    # load the data\n",
        "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        train_path,\n",
        "        label_mode=\"categorical\",\n",
        "        seed=123,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        batch_size=1)\n",
        "    \n",
        "    class_names = tuple(train_ds.class_names)\n",
        "    train_size = train_ds.cardinality().numpy()\n",
        "    train_ds = train_ds.unbatch().batch(int(BATCH_SIZE))\n",
        "    train_ds = train_ds.repeat()\n",
        "\n",
        "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        valid_path,\n",
        "        label_mode=\"categorical\",\n",
        "        seed=123,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        batch_size=1)\n",
        "    valid_size = val_ds.cardinality().numpy()\n",
        "    val_ds = val_ds.unbatch().batch(int(BATCH_SIZE)) \n",
        "\n",
        "    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        test_path,\n",
        "        label_mode=\"categorical\",\n",
        "        seed=123,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        batch_size=1)\n",
        "    \n",
        "\n",
        "    # normalization and data augmentation\n",
        "    normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
        "    preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
        "     \n",
        "    preprocessing_model.add(\n",
        "          tf.keras.layers.RandomRotation(40))\n",
        "    preprocessing_model.add(\n",
        "        tf.keras.layers.RandomTranslation(0, 0.2))\n",
        "    preprocessing_model.add(\n",
        "        tf.keras.layers.RandomTranslation(0.2, 0))\n",
        "    preprocessing_model.add(\n",
        "        tf.keras.layers.RandomZoom(0.2, 0.2))\n",
        "    preprocessing_model.add(\n",
        "        tf.keras.layers.RandomFlip(mode=\"horizontal\"))\n",
        "    preprocessing_model.add(\n",
        "        tf.keras.layers.RandomFlip(mode=\"vertical\"))\n",
        "\n",
        "    train_ds = train_ds.map(lambda images, labels:\n",
        "                        (preprocessing_model(images), labels))\n",
        "    val_ds = val_ds.map(lambda images, labels:\n",
        "                      (normalization_layer(images), labels))\n",
        "    test_ds = test_ds.map(lambda images, labels:\n",
        "                      (normalization_layer(images), labels))\n",
        "\n",
        "\n",
        "    # build the model\n",
        "    model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
        "    hub.KerasLayer(feature_extractor, trainable=False), # Here model is being downloaded\n",
        "    tf.keras.layers.Dropout(rate=0.1),\n",
        "    tf.keras.layers.Dense(len(class_names),\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "    ])\n",
        "    model.build((None,)+IMAGE_SIZE+(3,))\n",
        "    model.summary() \n",
        "    \n",
        "    # compile model\n",
        "    model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9), \n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    # Saving best model settings\n",
        "    SaveModelDir= f\"experiments/{model_name}/exp_for_{str(BATCH_SIZE)}_BatchSize\"\n",
        "    checkpoint_path = SaveModelDir+\"/cp.ckpt\"\n",
        "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "    metric = 'accuracy'\n",
        "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor=metric, \n",
        "                                                   save_weights_only=False, save_best_only = True, verbose=1)\n",
        "    # Save complete Model\n",
        "    model.save(SaveModelDir+f\"/model_for_{str(BATCH_SIZE)}_BatchSize\")\n",
        "\n",
        "\n",
        "    ############################################################################\n",
        "    # start timer for training \n",
        "    start_time = datetime.now()\n",
        "\n",
        "    steps_per_epoch = train_size // int(BATCH_SIZE)\n",
        "    validation_steps = valid_size // int(BATCH_SIZE)\n",
        "\n",
        "    hist = model.fit(\n",
        "      train_ds,\n",
        "      epochs=EPOCHS, steps_per_epoch=steps_per_epoch,\n",
        "      validation_data=val_ds,\n",
        "      validation_steps=validation_steps,\n",
        "      callbacks = [cp_callback]\n",
        "      ).history\n",
        "\n",
        "    end_time = datetime.now() # end timer\n",
        "\n",
        "    f = open(SaveModelDir+f\"/time_to_train_for_{BATCH_SIZE}_model.txt\", \"x\")\n",
        "    f.write('Duration: {}'.format(end_time - start_time))\n",
        "    f.close() \n",
        "    ############################################################################\n",
        "\n",
        "    # plot acc\n",
        "    graph_path = SaveModelDir+'/model_acc.png'\n",
        "    plot_acc(hist,graph_path)\n",
        "\n",
        "    # plot loss\n",
        "    graph_path = SaveModelDir+'/model_loss.png'\n",
        "    plot_loss(hist,graph_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Get metracis for val_ds \n",
        "    y_pred = []  \n",
        "    y_true = []\n",
        "\n",
        "    for image_batch, label_batch in val_ds:   \n",
        "      y_true.append(label_batch)\n",
        "      preds = model.predict(image_batch)\n",
        "      y_pred.append(np.argmax(preds, axis = - 1))\n",
        "    \n",
        "    correct_labels = tf.concat([item for item in y_true], axis = 0)\n",
        "    predicted_labels = tf.concat([item for item in y_pred], axis = 0)\n",
        "    # Un-One-hot encode the correct labels\n",
        "    correct_labels_tonums = np.argmax(correct_labels,axis=1)\n",
        "\n",
        "    cm = confusion_matrix(correct_labels_tonums, predicted_labels)\n",
        "    fig = plt.figure(figsize = (8,8))\n",
        "    ax1 = fig.add_subplot(1,1,1)\n",
        "    sns.set(font_scale=1.4) #for label size\n",
        "    sns.heatmap(cm, annot=True, annot_kws={\"size\": 12},\n",
        "      cbar = False, cmap='Purples');\n",
        "    ax1.set_ylabel('True Values',fontsize=14)\n",
        "    ax1.set_xlabel('Predicted Values',fontsize=14)\n",
        "    plt.savefig(SaveModelDir+'/con_matrx_val_ds.png')\n",
        "    plt.clf()\n",
        "\n",
        "    loss, acc = model.evaluate(val_ds, verbose=2)\n",
        "    f = open(SaveModelDir+f\"/VAL_DS_{BATCH_SIZE}_model.txt\", \"x\")\n",
        "    f.write(classification_report(correct_labels_tonums,predicted_labels))\n",
        "    f.write(\"\\n model accuracy on val_ds {:5.2f}%\".format(100 * acc))\n",
        "    f.close() \n",
        "\n",
        "\n",
        "\n",
        "    # Get metracis for test_ds\n",
        "    y_pred = []  \n",
        "    y_true = [] \n",
        "\n",
        "    for image_batch, label_batch in test_ds:   \n",
        "      y_true.append(label_batch)\n",
        "      preds = model.predict(image_batch)\n",
        "      y_pred.append(np.argmax(preds, axis = - 1))\n",
        "\n",
        "    # convert the true and predicted labels into tensors\n",
        "    correct_labels = tf.concat([item for item in y_true], axis = 0)\n",
        "    predicted_labels = tf.concat([item for item in y_pred], axis = 0)\n",
        "\n",
        "    # Un-One-hot encode the correct labels\n",
        "    correct_labels_tonums = np.argmax(correct_labels,axis=1)\n",
        "\n",
        "    \n",
        "    cm = confusion_matrix(correct_labels_tonums, predicted_labels)\n",
        "    fig = plt.figure(figsize = (8,8))\n",
        "    ax1 = fig.add_subplot(1,1,1)\n",
        "    sns.set(font_scale=1.4) #for label size\n",
        "    sns.heatmap(cm, annot=True, annot_kws={\"size\": 12},\n",
        "      cbar = False, cmap='Purples');\n",
        "    ax1.set_ylabel('True Values',fontsize=14)\n",
        "    ax1.set_xlabel('Predicted Values',fontsize=14)\n",
        "    plt.savefig(SaveModelDir+'/con_matrx_test_ds.png')\n",
        "    plt.clf()\n",
        "\n",
        "\n",
        "    loss, acc = model.evaluate(test_ds, verbose=2)\n",
        "    f = open(SaveModelDir+f\"/TEST_DS_{BATCH_SIZE}_model.txt\", \"x\")\n",
        "    f.write(classification_report(correct_labels_tonums,predicted_labels))\n",
        "    f.write(\"\\n model accuracy on test_ds {:5.2f}%\".format(100 * acc))\n",
        "    f.close() \n",
        "\n",
        "\n",
        "end_time = datetime.now() # end timer\n",
        "\n",
        "print(\"training 3 epochs of this script took : \" + 'Duration: {}'.format(end_time - start_time))"
      ],
      "metadata": {
        "id": "eXientSWzq80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6dd45b74-1c52-4c0f-e05e-1f992bfc3535"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 602 files belonging to 4 classes.\n",
            "Found 171 files belonging to 4 classes.\n",
            "Found 91 files belonging to 4 classes.\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_2 (KerasLayer)  (None, 1280)              2257984   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1280)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4)                 5124      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,263,108\n",
            "Trainable params: 5,124\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "18/18 [==============================] - ETA: 0s - loss: 1.2258 - accuracy: 0.4826\n",
            "Epoch 1: accuracy improved from -inf to 0.48264, saving model to experiments/mobilenet_v2/exp_for_32_BatchSize/cp.ckpt\n",
            "18/18 [==============================] - 12s 520ms/step - loss: 1.2258 - accuracy: 0.4826 - val_loss: 0.7934 - val_accuracy: 0.7500\n",
            "Epoch 2/3\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.6837 - accuracy: 0.8228\n",
            "Epoch 2: accuracy improved from 0.48264 to 0.82281, saving model to experiments/mobilenet_v2/exp_for_32_BatchSize/cp.ckpt\n",
            "18/18 [==============================] - 11s 648ms/step - loss: 0.6837 - accuracy: 0.8228 - val_loss: 0.6468 - val_accuracy: 0.8687\n",
            "Epoch 3/3\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.6110 - accuracy: 0.8825\n",
            "Epoch 3: accuracy improved from 0.82281 to 0.88246, saving model to experiments/mobilenet_v2/exp_for_32_BatchSize/cp.ckpt\n",
            "18/18 [==============================] - 8s 489ms/step - loss: 0.6110 - accuracy: 0.8825 - val_loss: 0.5823 - val_accuracy: 0.9062\n",
            "6/6 - 1s - loss: 0.5846 - accuracy: 0.9006 - 551ms/epoch - 92ms/step\n",
            "91/91 - 1s - loss: 0.5631 - accuracy: 0.9121 - 737ms/epoch - 8ms/step\n",
            "Found 602 files belonging to 4 classes.\n",
            "Found 171 files belonging to 4 classes.\n",
            "Found 91 files belonging to 4 classes.\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_3 (KerasLayer)  (None, 1280)              2257984   \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1280)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4)                 5124      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,263,108\n",
            "Trainable params: 5,124\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.3291 - accuracy: 0.4514\n",
            "Epoch 1: accuracy improved from -inf to 0.45139, saving model to experiments/mobilenet_v2/exp_for_48_BatchSize/cp.ckpt\n",
            "12/12 [==============================] - 12s 678ms/step - loss: 1.3291 - accuracy: 0.4514 - val_loss: 0.9744 - val_accuracy: 0.7083\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.8207 - accuracy: 0.7581\n",
            "Epoch 2: accuracy improved from 0.45139 to 0.75812, saving model to experiments/mobilenet_v2/exp_for_48_BatchSize/cp.ckpt\n",
            "12/12 [==============================] - 7s 611ms/step - loss: 0.8207 - accuracy: 0.7581 - val_loss: 0.7303 - val_accuracy: 0.7986\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6458 - accuracy: 0.8809\n",
            "Epoch 3: accuracy improved from 0.75812 to 0.88087, saving model to experiments/mobilenet_v2/exp_for_48_BatchSize/cp.ckpt\n",
            "12/12 [==============================] - 7s 596ms/step - loss: 0.6458 - accuracy: 0.8809 - val_loss: 0.6347 - val_accuracy: 0.8681\n",
            "4/4 - 0s - loss: 0.6398 - accuracy: 0.8596 - 454ms/epoch - 114ms/step\n",
            "91/91 - 1s - loss: 0.5755 - accuracy: 0.9011 - 753ms/epoch - 8ms/step\n",
            "Found 602 files belonging to 4 classes.\n",
            "Found 171 files belonging to 4 classes.\n",
            "Found 91 files belonging to 4 classes.\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_4 (KerasLayer)  (None, 1280)              2257984   \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 1280)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4)                 5124      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,263,108\n",
            "Trainable params: 5,124\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.4979 - accuracy: 0.3073\n",
            "Epoch 1: accuracy improved from -inf to 0.30729, saving model to experiments/mobilenet_v2/exp_for_64_BatchSize/cp.ckpt\n",
            "9/9 [==============================] - 11s 907ms/step - loss: 1.4979 - accuracy: 0.3073 - val_loss: 1.0923 - val_accuracy: 0.5391\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9122 - accuracy: 0.7026\n",
            "Epoch 2: accuracy improved from 0.30729 to 0.70260, saving model to experiments/mobilenet_v2/exp_for_64_BatchSize/cp.ckpt\n",
            "9/9 [==============================] - 8s 1s/step - loss: 0.9122 - accuracy: 0.7026 - val_loss: 0.7998 - val_accuracy: 0.7891\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6749 - accuracy: 0.8513\n",
            "Epoch 3: accuracy improved from 0.70260 to 0.85130, saving model to experiments/mobilenet_v2/exp_for_64_BatchSize/cp.ckpt\n",
            "9/9 [==============================] - 6s 795ms/step - loss: 0.6749 - accuracy: 0.8513 - val_loss: 0.6823 - val_accuracy: 0.8516\n",
            "3/3 - 1s - loss: 0.6569 - accuracy: 0.8713 - 912ms/epoch - 304ms/step\n",
            "91/91 - 1s - loss: 0.6200 - accuracy: 0.8901 - 747ms/epoch - 8ms/step\n",
            "Found 602 files belonging to 4 classes.\n",
            "Found 171 files belonging to 4 classes.\n",
            "Found 91 files belonging to 4 classes.\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_5 (KerasLayer)  (None, 1024)              5607184   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 4)                 4100      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,611,284\n",
            "Trainable params: 4,100\n",
            "Non-trainable params: 5,607,184\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "18/18 [==============================] - ETA: 0s - loss: 1.4210 - accuracy: 0.2847\n",
            "Epoch 1: accuracy improved from -inf to 0.28472, saving model to experiments/inception_v1/exp_for_32_BatchSize/cp.ckpt\n",
            "18/18 [==============================] - 11s 422ms/step - loss: 1.4210 - accuracy: 0.2847 - val_loss: 1.1239 - val_accuracy: 0.5437\n",
            "Epoch 2/3\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.9867 - accuracy: 0.6842\n",
            "Epoch 2: accuracy improved from 0.28472 to 0.68421, saving model to experiments/inception_v1/exp_for_32_BatchSize/cp.ckpt\n",
            "18/18 [==============================] - 7s 366ms/step - loss: 0.9867 - accuracy: 0.6842 - val_loss: 0.8619 - val_accuracy: 0.8062\n",
            "Epoch 3/3\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.8087 - accuracy: 0.8088\n",
            "Epoch 3: accuracy improved from 0.68421 to 0.80877, saving model to experiments/inception_v1/exp_for_32_BatchSize/cp.ckpt\n",
            "18/18 [==============================] - 7s 402ms/step - loss: 0.8087 - accuracy: 0.8088 - val_loss: 0.7667 - val_accuracy: 0.8438\n",
            "6/6 - 0s - loss: 0.7591 - accuracy: 0.8480 - 439ms/epoch - 73ms/step\n",
            "91/91 - 1s - loss: 0.7434 - accuracy: 0.8791 - 869ms/epoch - 10ms/step\n",
            "Found 602 files belonging to 4 classes.\n",
            "Found 171 files belonging to 4 classes.\n",
            "Found 91 files belonging to 4 classes.\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_6 (KerasLayer)  (None, 1024)              5607184   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 4)                 4100      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,611,284\n",
            "Trainable params: 4,100\n",
            "Non-trainable params: 5,607,184\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.3740 - accuracy: 0.4219\n",
            "Epoch 1: accuracy improved from -inf to 0.42188, saving model to experiments/inception_v1/exp_for_48_BatchSize/cp.ckpt\n",
            "12/12 [==============================] - 11s 621ms/step - loss: 1.3740 - accuracy: 0.4219 - val_loss: 1.1896 - val_accuracy: 0.4028\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.1073 - accuracy: 0.5162\n",
            "Epoch 2: accuracy improved from 0.42188 to 0.51625, saving model to experiments/inception_v1/exp_for_48_BatchSize/cp.ckpt\n",
            "12/12 [==============================] - 6s 561ms/step - loss: 1.1073 - accuracy: 0.5162 - val_loss: 0.9960 - val_accuracy: 0.7222\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.9209 - accuracy: 0.7401\n",
            "Epoch 3: accuracy improved from 0.51625 to 0.74007, saving model to experiments/inception_v1/exp_for_48_BatchSize/cp.ckpt\n",
            "12/12 [==============================] - 8s 705ms/step - loss: 0.9209 - accuracy: 0.7401 - val_loss: 0.8838 - val_accuracy: 0.7569\n",
            "4/4 - 0s - loss: 0.8724 - accuracy: 0.7719 - 460ms/epoch - 115ms/step\n",
            "91/91 - 1s - loss: 0.8306 - accuracy: 0.8132 - 911ms/epoch - 10ms/step\n",
            "Found 602 files belonging to 4 classes.\n",
            "Found 171 files belonging to 4 classes.\n",
            "Found 91 files belonging to 4 classes.\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_7 (KerasLayer)  (None, 1024)              5607184   \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 4)                 4100      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,611,284\n",
            "Trainable params: 4,100\n",
            "Non-trainable params: 5,607,184\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.4726 - accuracy: 0.2691\n",
            "Epoch 1: accuracy improved from -inf to 0.26910, saving model to experiments/inception_v1/exp_for_64_BatchSize/cp.ckpt\n",
            "9/9 [==============================] - 11s 841ms/step - loss: 1.4726 - accuracy: 0.2691 - val_loss: 1.2845 - val_accuracy: 0.3594\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.2326 - accuracy: 0.4498\n",
            "Epoch 2: accuracy improved from 0.26910 to 0.44981, saving model to experiments/inception_v1/exp_for_64_BatchSize/cp.ckpt\n",
            "9/9 [==============================] - 6s 758ms/step - loss: 1.2326 - accuracy: 0.4498 - val_loss: 1.1386 - val_accuracy: 0.5312\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0486 - accuracy: 0.6004\n",
            "Epoch 3: accuracy improved from 0.44981 to 0.60037, saving model to experiments/inception_v1/exp_for_64_BatchSize/cp.ckpt\n",
            "9/9 [==============================] - 7s 815ms/step - loss: 1.0486 - accuracy: 0.6004 - val_loss: 0.9787 - val_accuracy: 0.6797\n",
            "3/3 - 2s - loss: 0.9672 - accuracy: 0.6842 - 2s/epoch - 541ms/step\n",
            "91/91 - 1s - loss: 0.9124 - accuracy: 0.7473 - 950ms/epoch - 10ms/step\n",
            "Found 602 files belonging to 4 classes.\n",
            "Found 171 files belonging to 4 classes.\n",
            "Found 91 files belonging to 4 classes.\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_8 (KerasLayer)  (None, 2048)              21802784  \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 4)                 8196      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,810,980\n",
            "Trainable params: 8,196\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "18/18 [==============================] - ETA: 0s - loss: 1.1233 - accuracy: 0.5642\n",
            "Epoch 1: accuracy improved from -inf to 0.56424, saving model to experiments/inception_v3_inaturalist/exp_for_32_BatchSize/cp.ckpt\n",
            "18/18 [==============================] - 18s 702ms/step - loss: 1.1233 - accuracy: 0.5642 - val_loss: 0.7038 - val_accuracy: 0.9000\n",
            "Epoch 2/3\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.6289 - accuracy: 0.8772\n",
            "Epoch 2: accuracy improved from 0.56424 to 0.87719, saving model to experiments/inception_v3_inaturalist/exp_for_32_BatchSize/cp.ckpt\n",
            "18/18 [==============================] - 13s 646ms/step - loss: 0.6289 - accuracy: 0.8772 - val_loss: 0.5647 - val_accuracy: 0.9187\n",
            "Epoch 3/3\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.5566 - accuracy: 0.9035\n",
            "Epoch 3: accuracy improved from 0.87719 to 0.90351, saving model to experiments/inception_v3_inaturalist/exp_for_32_BatchSize/cp.ckpt\n",
            "18/18 [==============================] - 11s 656ms/step - loss: 0.5566 - accuracy: 0.9035 - val_loss: 0.5185 - val_accuracy: 0.9438\n",
            "6/6 - 1s - loss: 0.5227 - accuracy: 0.9415 - 794ms/epoch - 132ms/step\n",
            "91/91 - 2s - loss: 0.5037 - accuracy: 0.9560 - 2s/epoch - 17ms/step\n",
            "Found 602 files belonging to 4 classes.\n",
            "Found 171 files belonging to 4 classes.\n",
            "Found 91 files belonging to 4 classes.\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_9 (KerasLayer)  (None, 2048)              21802784  \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 4)                 8196      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,810,980\n",
            "Trainable params: 8,196\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.2012 - accuracy: 0.5226\n",
            "Epoch 1: accuracy improved from -inf to 0.52257, saving model to experiments/inception_v3_inaturalist/exp_for_48_BatchSize/cp.ckpt\n",
            "12/12 [==============================] - 19s 1s/step - loss: 1.2012 - accuracy: 0.5226 - val_loss: 0.8829 - val_accuracy: 0.7222\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7491 - accuracy: 0.8141\n",
            "Epoch 2: accuracy improved from 0.52257 to 0.81408, saving model to experiments/inception_v3_inaturalist/exp_for_48_BatchSize/cp.ckpt\n",
            "12/12 [==============================] - 11s 1s/step - loss: 0.7491 - accuracy: 0.8141 - val_loss: 0.6375 - val_accuracy: 0.8889\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.5875 - accuracy: 0.8881\n",
            "Epoch 3: accuracy improved from 0.81408 to 0.88809, saving model to experiments/inception_v3_inaturalist/exp_for_48_BatchSize/cp.ckpt\n",
            "12/12 [==============================] - 11s 965ms/step - loss: 0.5875 - accuracy: 0.8881 - val_loss: 0.5578 - val_accuracy: 0.9236\n",
            "4/4 - 1s - loss: 0.5579 - accuracy: 0.9181 - 881ms/epoch - 220ms/step\n",
            "91/91 - 1s - loss: 0.5546 - accuracy: 0.9560 - 1s/epoch - 16ms/step\n",
            "Found 602 files belonging to 4 classes.\n",
            "Found 171 files belonging to 4 classes.\n",
            "Found 91 files belonging to 4 classes.\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_10 (KerasLayer)  (None, 2048)             21802784  \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 4)                 8196      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,810,980\n",
            "Trainable params: 8,196\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.4213 - accuracy: 0.3403\n",
            "Epoch 1: accuracy improved from -inf to 0.34028, saving model to experiments/inception_v3_inaturalist/exp_for_64_BatchSize/cp.ckpt\n",
            "9/9 [==============================] - 19s 1s/step - loss: 1.4213 - accuracy: 0.3403 - val_loss: 1.0585 - val_accuracy: 0.6406\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.8940 - accuracy: 0.7491\n",
            "Epoch 2: accuracy improved from 0.34028 to 0.74907, saving model to experiments/inception_v3_inaturalist/exp_for_64_BatchSize/cp.ckpt\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.8940 - accuracy: 0.7491 - val_loss: 0.7068 - val_accuracy: 0.8125\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6355 - accuracy: 0.8773\n",
            "Epoch 3: accuracy improved from 0.74907 to 0.87732, saving model to experiments/inception_v3_inaturalist/exp_for_64_BatchSize/cp.ckpt\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.6355 - accuracy: 0.8773 - val_loss: 0.5790 - val_accuracy: 0.9141\n",
            "3/3 - 3s - loss: 0.5884 - accuracy: 0.9181 - 3s/epoch - 1s/step\n",
            "91/91 - 2s - loss: 0.5826 - accuracy: 0.9121 - 2s/epoch - 20ms/step\n",
            "Found 602 files belonging to 4 classes.\n",
            "Found 171 files belonging to 4 classes.\n",
            "Found 91 files belonging to 4 classes.\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_11 (KerasLayer)  (None, 1536)             54336736  \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 1536)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 4)                 6148      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54,342,884\n",
            "Trainable params: 6,148\n",
            "Non-trainable params: 54,336,736\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "18/18 [==============================] - ETA: 0s - loss: 1.3906 - accuracy: 0.3628\n",
            "Epoch 1: accuracy improved from -inf to 0.36285, saving model to experiments/inception_resnet_v2/exp_for_32_BatchSize/cp.ckpt\n",
            "18/18 [==============================] - 41s 2s/step - loss: 1.3906 - accuracy: 0.3628 - val_loss: 1.0529 - val_accuracy: 0.7125\n",
            "Epoch 2/3\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.9094 - accuracy: 0.7316\n",
            "Epoch 2: accuracy improved from 0.36285 to 0.73158, saving model to experiments/inception_resnet_v2/exp_for_32_BatchSize/cp.ckpt\n",
            "18/18 [==============================] - 28s 2s/step - loss: 0.9094 - accuracy: 0.7316 - val_loss: 0.8000 - val_accuracy: 0.8062\n",
            "Epoch 3/3\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.7526 - accuracy: 0.8263\n",
            "Epoch 3: accuracy improved from 0.73158 to 0.82632, saving model to experiments/inception_resnet_v2/exp_for_32_BatchSize/cp.ckpt\n",
            "18/18 [==============================] - 27s 2s/step - loss: 0.7526 - accuracy: 0.8263 - val_loss: 0.7143 - val_accuracy: 0.8625\n",
            "6/6 - 2s - loss: 0.7132 - accuracy: 0.8596 - 2s/epoch - 329ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:185: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91/91 - 3s - loss: 0.6778 - accuracy: 0.8791 - 3s/epoch - 33ms/step\n",
            "Found 602 files belonging to 4 classes.\n",
            "Found 171 files belonging to 4 classes.\n",
            "Found 91 files belonging to 4 classes.\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_12 (KerasLayer)  (None, 1536)             54336736  \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 1536)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 4)                 6148      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54,342,884\n",
            "Trainable params: 6,148\n",
            "Non-trainable params: 54,336,736\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.3309 - accuracy: 0.3698\n",
            "Epoch 1: accuracy improved from -inf to 0.36979, saving model to experiments/inception_resnet_v2/exp_for_48_BatchSize/cp.ckpt\n",
            "12/12 [==============================] - 41s 2s/step - loss: 1.3309 - accuracy: 0.3698 - val_loss: 1.1400 - val_accuracy: 0.5972\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.0219 - accuracy: 0.6859\n",
            "Epoch 2: accuracy improved from 0.36979 to 0.68592, saving model to experiments/inception_resnet_v2/exp_for_48_BatchSize/cp.ckpt\n",
            "12/12 [==============================] - 26s 2s/step - loss: 1.0219 - accuracy: 0.6859 - val_loss: 0.9007 - val_accuracy: 0.7986\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.8352 - accuracy: 0.7942\n",
            "Epoch 3: accuracy improved from 0.68592 to 0.79422, saving model to experiments/inception_resnet_v2/exp_for_48_BatchSize/cp.ckpt\n",
            "12/12 [==============================] - 27s 2s/step - loss: 0.8352 - accuracy: 0.7942 - val_loss: 0.7732 - val_accuracy: 0.8611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:149: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 - 2s - loss: 0.7678 - accuracy: 0.8655 - 2s/epoch - 470ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:185: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91/91 - 3s - loss: 0.7205 - accuracy: 0.8901 - 3s/epoch - 34ms/step\n",
            "Found 602 files belonging to 4 classes.\n",
            "Found 171 files belonging to 4 classes.\n",
            "Found 91 files belonging to 4 classes.\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_13 (KerasLayer)  (None, 1536)             54336736  \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 1536)              0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 4)                 6148      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54,342,884\n",
            "Trainable params: 6,148\n",
            "Non-trainable params: 54,336,736\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.3897 - accuracy: 0.2865\n",
            "Epoch 1: accuracy improved from -inf to 0.28646, saving model to experiments/inception_resnet_v2/exp_for_64_BatchSize/cp.ckpt\n",
            "9/9 [==============================] - 42s 3s/step - loss: 1.3897 - accuracy: 0.2865 - val_loss: 1.2214 - val_accuracy: 0.5234\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.1257 - accuracy: 0.6041\n",
            "Epoch 2: accuracy improved from 0.28646 to 0.60409, saving model to experiments/inception_resnet_v2/exp_for_64_BatchSize/cp.ckpt\n",
            "9/9 [==============================] - 27s 3s/step - loss: 1.1257 - accuracy: 0.6041 - val_loss: 0.9891 - val_accuracy: 0.7734\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9228 - accuracy: 0.7528\n",
            "Epoch 3: accuracy improved from 0.60409 to 0.75279, saving model to experiments/inception_resnet_v2/exp_for_64_BatchSize/cp.ckpt\n",
            "9/9 [==============================] - 27s 3s/step - loss: 0.9228 - accuracy: 0.7528 - val_loss: 0.8683 - val_accuracy: 0.7812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:149: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 - 4s - loss: 0.8385 - accuracy: 0.8070 - 4s/epoch - 1s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:185: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91/91 - 3s - loss: 0.8268 - accuracy: 0.8242 - 3s/epoch - 33ms/step\n",
            "training 3 epochs of this script took : Duration: 0:02:00.767600\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}